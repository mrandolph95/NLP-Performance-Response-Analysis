# -*- coding: utf-8 -*-
"""Download and Save Comments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-8cFXsw16n-E8maol7RGkBdG5BnFLC1
"""

import pandas as pd
import csv
import re
import matplotlib, seaborn, wordcloud, nltk
import numpy as np
import requests
import json
import os
from tqdm import tqdm

from google.colab import userdata
from google.colab import drive

pip install google-api-python-client

from googleapiclient.discovery import build

youtube_api = userdata.get("YouTubeAPI")

youtube = build("youtube", "v3", developerKey=youtube_api)

"""#### Grab and save YouTube comments from each video"""

# URLS
youtube_beyonce_url = "https://www.youtube.com/watch?v=SDPITj1wlkg"
youtube_kendrick_lamar_url = "https://www.youtube.com/watch?v=KDorKy-13ak"

# Extract video IDs
videoID_beyonce = youtube_beyonce_url.split("v=")[1]
videoID_kendrick = youtube_kendrick_lamar_url.split("v=")[1]

def get_YT_comments(video_id):
    comments_data = []
    next_page_token = None

    while True:
        response = youtube.commentThreads().list(
            part='snippet',
            videoId=video_id,
            pageToken=next_page_token,
            textFormat="plainText",
            maxResults=100  # optional: speeds up data retrieval in larger chunks
        ).execute()

        for item in response['items']:
            snippet = item['snippet']['topLevelComment']['snippet']
            comment_info = {
                'video_id': video_id,
                'comment_id': item['id'],
                'comment': snippet.get('textDisplay'),
                'like_count': snippet.get('likeCount'),
                'reply_count': item['snippet'].get('totalReplyCount'),
                'published_at': snippet.get('publishedAt')
            }
            comments_data.append(comment_info)

        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break

    return comments_data

'''
# Get YouTube comments
def get_YT_comments(video_id):
    comments = []
    next_page_token = None

    while True:
        response = youtube.commentThreads().list(
            part='snippet',
            videoId=video_id,
            pageToken=next_page_token,
            textFormat="plainText"
        ).execute()

        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)

        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break

    return comments
    '''

# Beyonce Superbowl Performance
comments_beyonce = get_YT_comments(videoID_beyonce)
# Kendrick Lamar Performance
comments_kendrick = get_YT_comments(videoID_kendrick)

print(comments_kendrick[:5])

# Put comments in data frame
beyonce_sentiment = pd.DataFrame(comments_beyonce)

kendrick_sentiment = pd.DataFrame(comments_kendrick)

beyonce_sentiment.head()

kendrick_sentiment.head()

# Save data frames to CSV files
kendrick_sentiment.to_csv("/content/drive/MyDrive/kendrick_sentiment.csv")
beyonce_sentiment.to_csv("/content/drive/MyDrive/beyonce_sentiment.csv")

print(f'Kendrick Comments: {len(kendrick_sentiment)}')
print(f'Beyonce Comments: {len(beyonce_sentiment)}')

sampled = kendrick_sentiment.sample(n=3000, random_state=42).copy()
sampled['label'] = ""
sampled['confidence'] = ""
sampled.reset_index(drop=True, inplace=True)
sampled['comment_id'] = sampled.index
sampled.to_csv('/content/drive/MyDrive/kcomments_to_label.csv', index=False)

"""#### Analysis"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np

# Load model and tokenizer
MODEL = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

# Sentiment labels
labels = ['negative', 'neutral', 'positive']

def get_sentiment(text):
    # Tokenize and truncate long comments
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    scores = outputs[0][0].numpy()
    scores = np.exp(scores) / np.sum(np.exp(scores))  # Softmax

    sentiment = labels[np.argmax(scores)]
    return sentiment, scores

tqdm.pandas()

kendrick_sentiment['Sentiment_Result'] = kendrick_sentiment['Comments'].progress_apply(lambda x: get_sentiment(str(x))[0])
kendrick_sentiment['Sentiment_Scores'] = kendrick_sentiment['Comments'].progress_apply(lambda x: get_sentiment(str(x))[1])

tqdm.pandas()

# Add columns for sentiment and raw scores
beyonce_sentiment['Sentiment_Result'] = beyonce_sentiment['Comments'].progress_apply(lambda x: get_sentiment(str(x))[0])
beyonce_sentiment['Sentiment_Scores'] = beyonce_sentiment['Comments'].progress_apply(lambda x: get_sentiment(str(x))[1])

beyonce_sentiment.head()

beyonce_sentiment.to_csv("/content/drive/MyDrive/beyonce_sentiment_results.csv", index=False)

def samples(df):
  df['label'] = ""
  df.reset_index(drop=True, inplace=True)
  df['comment_id'] = df.index
  df.drop_duplicates(subset=['comment_id'], inplace=True)

#drive.flush_and_unmount()
drive.mount('/content/drive')

